# SmolLM2-135M Model Card

## Model Details
**SmolLM2-135M** is a compact 135 million parameter language model designed for efficiency and educational purposes. It is part of the SmolLM2 family of models, known for being "blazingly fast and remarkably powerful" for their size.

- **Developer:** Antigravity (Implementation based on Hugging Face SmolLM2 paper)
- **Model Type:** Decoder-only Transformer (Llama architecture)
- **Parameters:** ~135M
- **Context Length:** 2048 tokens
- **Architecture:** Dense (non-MoE) with Grouped Query Attention (GQA) and SwiGLU activations.

## Architecture
The model uses a modernized Llama architecture optimized for small scale:
- **Layers:** 30
- **Hidden Dimension (`d_model`):** 576
- **MLP Dimension (`d_ff`):** 1536 (SwiGLU)
- **Attention Heads:** 9
- **KV Heads:** 3 (GQA group size 3)
- **Vocabulary:** 49,152

## Training
This implementation supports training on custom datasets using the provided trainer.

### Performance Comparison
We compared the dense SmolLM2-135M against a 160M parameter Mixture-of-Experts (MoE) baseline.

![SmolLM2 vs Baseline](smollm2_comparison.png)

*Figure 1: Validation loss comparison over a short 500-step training run.*

**Observations:**
- **Speed:** SmolLM2-135M is approximately **23% faster** to train than the MoE baseline due to the efficiency of Grouped Query Attention (GQA) and dense matrix operations.
- **Convergence:** While it starts with higher loss (typical for deeper dense models compared to shallow MoEs), it shows steady convergence.

## Usage
```python
import torch
from configs.smollm2_135m_config import SmolLM2_135M_Config
from models.llm import MoEMinimalLLM

# Initialize config
config = SmolLM2_135M_Config()

# Initialize model
model = MoEMinimalLLM(config)

# Forward pass
input_ids = torch.randint(0, config.vocab_size, (1, 32))
logits = model(input_ids)
print(logits.shape) # torch.Size([1, 32, 49152])
```

## Intended Use
- Educational experimentation with SLMs (Small Language Models).
- Testing architectural optimizations like GQA on consumer hardware.
- Low-latency inference applications.

## Citation
If you use this implementation, please cite the original SmolLM2 paper:
> Allal, L. B., et al. "SmolLM2: When Smol Goes Big â€” Data-Centric Training of a Small Language Model". arXiv:2502.02737. 2025.
